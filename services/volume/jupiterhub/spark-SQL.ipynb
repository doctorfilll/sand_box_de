{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7879a40d-3ea5-4596-b5f5-f5e3f9921f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.19 pyspark-shell'\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder.config(\"spark.driver.host\", \"localhost\") \\\n",
    "  .appName(\"Spark SQL\") \\\n",
    "  .master(\"local\") \\\n",
    "  .enableHiveSupport() \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696bb2fa-9ac6-4427-96d4-e038abb9f813",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/notebooks/data/cars.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cars_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/cars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mpersist(StorageLevel\u001b[38;5;241m.\u001b[39mMEMORY_ONLY)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/notebooks/data/cars."
     ]
    }
   ],
   "source": [
    "cars_df = spark \\\n",
    "  .read \\\n",
    "  .json(\"data/cars\") \\\n",
    "  .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab96f477-4839-44bd-83ec-56f97c1253d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcars_df\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m) \u001b[38;5;66;03m# показать 10 строк, обрубать длинные строки по 30 символу\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cars_df' is not defined"
     ]
    }
   ],
   "source": [
    "cars_df.show(10, 30) # показать 10 строк, обрубать длинные строки по 30 символу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b3de4d-bc6f-44aa-9967-1888de201ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark DSL\n",
    "american_cars_df = cars_df \\\n",
    "  .filter(col(\"Origin\") == \"Japan\") \\\n",
    "  .select(col(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965cdb6e-5209-42f2-8e9d-022ad0132709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Name#13]\n",
      "   +- Filter (isnotnull(Origin#14) AND (Origin#14 = Japan))\n",
      "      +- InMemoryTableScan [Name#13, Origin#14], [isnotnull(Origin#14), (Origin#14 = Japan)]\n",
      "            +- InMemoryRelation [Acceleration#8, Cylinders#9L, Displacement#10, Horsepower#11L, Miles_per_Gallon#12, Name#13, Origin#14, Weight_in_lbs#15L, Year#16], StorageLevel(memory, 1 replicas)\n",
      "                  +- FileScan json [Acceleration#8,Cylinders#9L,Displacement#10,Horsepower#11L,Miles_per_Gallon#12,Name#13,Origin#14,Weight_in_lbs#15L,Year#16] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/notebooks/data/cars], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Acceleration:double,Cylinders:bigint,Displacement:double,Horsepower:bigint,Miles_per_Gallo...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "american_cars_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be82357d-8f8d-44d6-a69a-719a960d2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|Name                       |\n",
      "+---------------------------+\n",
      "|toyota corona mark ii      |\n",
      "|datsun pl510               |\n",
      "|datsun pl510               |\n",
      "|toyota corona              |\n",
      "|toyota corolla 1200        |\n",
      "|datsun 1200                |\n",
      "|toyota corona hardtop      |\n",
      "|mazda rx2 coupe            |\n",
      "|datsun 510 (sw)            |\n",
      "|toyouta corona mark ii (sw)|\n",
      "+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "american_cars_df.show(10, False) # показать 10 строк, не обрубать длинные строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131f2eb6-f9c6-4b5b-b81f-82d8df61061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame => SQL metastore = EXTERNAL TABLE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='cars', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранить как таблицу в Spark, но не сохранять данные на диск\n",
    "#  DataFrame => SQL metastore\n",
    "print(\"DataFrame => SQL metastore = EXTERNAL TABLE\")\n",
    "cars_df.createOrReplaceTempView(\"cars\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0085418a-83af-49a9-8923-e6f66bd068a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|Name                       |\n",
      "+---------------------------+\n",
      "|toyota corona mark ii      |\n",
      "|datsun pl510               |\n",
      "|datsun pl510               |\n",
      "|toyota corona              |\n",
      "|toyota corolla 1200        |\n",
      "|datsun 1200                |\n",
      "|toyota corona hardtop      |\n",
      "|mazda rx2 coupe            |\n",
      "|datsun 510 (sw)            |\n",
      "|toyouta corona mark ii (sw)|\n",
      "+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL != Spark DSL\n",
    "# Выполнить SQL запросы к DF, которые известны Apache Spark под какими-то именами\n",
    "american_cars_df_v2 = spark.sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "american_cars_df_v2.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22044fb4-be96-47b9-bc70-064510d5fd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Name#13]\n",
      "   +- Filter (isnotnull(Origin#14) AND (Origin#14 = Japan))\n",
      "      +- InMemoryTableScan [Name#13, Origin#14], [isnotnull(Origin#14), (Origin#14 = Japan)]\n",
      "            +- InMemoryRelation [Acceleration#8, Cylinders#9L, Displacement#10, Horsepower#11L, Miles_per_Gallon#12, Name#13, Origin#14, Weight_in_lbs#15L, Year#16], StorageLevel(memory, 1 replicas)\n",
      "                  +- FileScan json [Acceleration#8,Cylinders#9L,Displacement#10,Horsepower#11L,Miles_per_Gallon#12,Name#13,Origin#14,Weight_in_lbs#15L,Year#16] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/notebooks/data/cars], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Acceleration:double,Cylinders:bigint,Displacement:double,Horsepower:bigint,Miles_per_Gallo...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "american_cars_df_v2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacaf955-0b74-490d-ba0d-aa79e00125d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cars` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 17;\n",
      "'Project ['Name]\n",
      "+- 'Filter ('Origin = Japan)\n",
      "   +- 'UnresolvedRelation [cars], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Временные таблицы действуют только на время жизни сессии\n",
    "try:\n",
    "  spark.newSession().sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "except Exception as e:\n",
    "  print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d8dd6c-2164-4aad-8437-601bd037c8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удаление внешней таблицы (External Table) не удаляет данные на диске, только в metastore\n",
    "spark.sql(\"DROP TABLE cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5930653-8134-44dd-b6ae-30a6b72cc2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='cars_managed_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39c4033-e7ca-45ea-a33f-f4289592adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ожидаемо падает:\n",
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cars` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 17;\n",
      "'Project ['Name]\n",
      "+- 'Filter ('Origin = Japan)\n",
      "   +- 'UnresolvedRelation [cars], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ожидаемо падает:\")\n",
    "try:\n",
    "  spark.sql(\"SELECT Name FROM cars WHERE Origin = 'Japan'\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b470c4f3-bba4-4a25-be2a-79daabbc0107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Но данные по прежнему доступны для чтения с диска\n",
    "cars_df_again = spark.read.json(\"data/cars\")\n",
    "cars_df_again.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cb3c6-fc40-4830-a546-f2c58446f845",
   "metadata": {},
   "source": [
    "Сохранить датафрейм как Spark таблицу: `DataFrame => SQL metastore + Spark storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b739f79b-af84-40f0-a122-5d7436c3bf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "!find ~ -type d -name cars_managed_table | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b144541-341b-4dec-8ca6-91a08f4e6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df \\\n",
    "  .write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"cars_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49ac388-cb79-482d-b768-0e20b77ee65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/spark-warehouse/cars_managed_table\n",
      "/home/jovyan/notebooks/spark-warehouse/cars_managed_table/.part-00000-a9752fb9-7b8a-4959-9a50-e4806ab5715f-c000.snappy.parquet.crc\n",
      "/home/jovyan/notebooks/spark-warehouse/cars_managed_table/._SUCCESS.crc\n",
      "/home/jovyan/notebooks/spark-warehouse/cars_managed_table/part-00000-a9752fb9-7b8a-4959-9a50-e4806ab5715f-c000.snappy.parquet\n",
      "/home/jovyan/notebooks/spark-warehouse/cars_managed_table/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!find ~ -type d -name cars_managed_table -exec find {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538ee6b-99d7-4639-8660-531ccac9509c",
   "metadata": {},
   "source": [
    "`saveAsTable` выполняет действия отличные от `save()` + `orc(...)` или `parquet(....)`.\n",
    "\n",
    "В случае, например, `parquet()` указывается место для хранения файлов на диске:\n",
    "```python\n",
    "df.write \\\n",
    "  .parquet(\"data/parquet\"). \\\n",
    "  save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c541eb9-407e-4bf2-8f55-7877183d4dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтение из управляемой таблицы (Managed Table) при помощи SQL запроса:\n",
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|Name                     |Origin|Weight_in_lbs|Year      |\n",
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "|12.0        |8        |307.0       |130       |18.0            |chevrolet chevelle malibu|USA   |3504         |NULL      |\n",
      "|12.0        |8        |307.0       |130       |18.0            |chevrolet chevelle malibu|USA   |3504         |1970-01-01|\n",
      "|11.5        |8        |350.0       |165       |15.0            |buick skylark 320        |USA   |3693         |1970-01-01|\n",
      "|11.0        |8        |318.0       |150       |18.0            |plymouth satellite       |USA   |3436         |1970-01-01|\n",
      "|12.0        |8        |304.0       |150       |16.0            |amc rebel sst            |USA   |3433         |1970-01-01|\n",
      "|10.5        |8        |302.0       |140       |17.0            |ford torino              |USA   |3449         |1970-01-01|\n",
      "|10.0        |8        |429.0       |198       |15.0            |ford galaxie 500         |USA   |4341         |1970-01-01|\n",
      "|9.0         |8        |454.0       |220       |14.0            |chevrolet impala         |USA   |4354         |1970-01-01|\n",
      "|8.5         |8        |440.0       |215       |14.0            |plymouth fury iii        |USA   |4312         |1970-01-01|\n",
      "|10.0        |8        |455.0       |225       |14.0            |pontiac catalina         |USA   |4425         |1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Чтение из управляемой таблицы (Managed Table) при помощи SQL запроса:\")\n",
    "american_cars_df_v2 = spark.sql(\"SELECT * FROM cars_managed_table\")\n",
    "american_cars_df_v2.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb1303b3-36af-479d-9423-8beb40d1211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(cars_df.count() == american_cars_df_v2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7eed6af-8214-46f2-9f59-a3529253fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.table == spark.read.table\n",
    "# cars_managed_df = spark.read.table(\"cars_managed_table\")\n",
    "\n",
    "print(\"Чтение из управляемой таблицы (Managed Table) при помощи Spark DSL:\")\n",
    "\n",
    "cars_managed_df = spark.table(\"cars_managed_table\")\n",
    "assert (cars_managed_df.count() != 0)\n",
    "cars_managed_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fa7d24-4c5e-421a-8a17-eb6acb949d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|      NULL|\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
      "|        11.0|        8|       318.0|       150|            18.0|  plymouth satellite|   USA|         3436|1970-01-01|\n",
      "|        12.0|        8|       304.0|       150|            16.0|       amc rebel sst|   USA|         3433|1970-01-01|\n",
      "|        10.5|        8|       302.0|       140|            17.0|         ford torino|   USA|         3449|1970-01-01|\n",
      "|        10.0|        8|       429.0|       198|            15.0|    ford galaxie 500|   USA|         4341|1970-01-01|\n",
      "|         9.0|        8|       454.0|       220|            14.0|    chevrolet impala|   USA|         4354|1970-01-01|\n",
      "|         8.5|        8|       440.0|       215|            14.0|   plymouth fury iii|   USA|         4312|1970-01-01|\n",
      "|        10.0|        8|       455.0|       225|            14.0|    pontiac catalina|   USA|         4425|1970-01-01|\n",
      "|         8.5|        8|       390.0|       190|            15.0|  amc ambassador dpl|   USA|         3850|1970-01-01|\n",
      "|        17.5|        4|       133.0|       115|            NULL|citroen ds-21 pallas|Europe|         3090|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            NULL|chevrolet chevell...|   USA|         4142|1970-01-01|\n",
      "|        11.0|        8|       351.0|       153|            NULL|    ford torino (sw)|   USA|         4034|1970-01-01|\n",
      "|        10.5|        8|       383.0|       175|            NULL|plymouth satellit...|   USA|         4166|1970-01-01|\n",
      "|        11.0|        8|       360.0|       175|            NULL|  amc rebel sst (sw)|   USA|         3850|1970-01-01|\n",
      "|        10.0|        8|       383.0|       170|            15.0| dodge challenger se|   USA|         3563|1970-01-01|\n",
      "|         8.0|        8|       340.0|       160|            14.0|   plymouth cuda 340|   USA|         3609|1970-01-01|\n",
      "|         8.0|        8|       302.0|       140|            NULL|ford mustang boss...|   USA|         3353|1970-01-01|\n",
      "|         9.5|        8|       400.0|       150|            15.0|chevrolet monte c...|   USA|         3761|1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"SELECT * FROM cars_managed_table\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "234b702b-a0b5-4c78-b5b8-51fa03e66191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалить управляемую таблицу (Managed Table)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Удалить управляемую таблицу (Managed Table)\")\n",
    "spark.sql(\"DROP TABLE cars_managed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c807170a-2019-4d7a-803e-584cca6a7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "!find ~ -type d -name cars_managed_table | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14bc5aab-e4eb-425e-b2bb-d5a4ee4fb838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/home/jovyan/notebooks/spark-warehouse'),\n",
       " Database(name='test', catalog='spark_catalog', description='', locationUri='file:/home/jovyan/notebooks/spark-warehouse/test.db')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d154dd-b434-47e7-947b-3c60501f11e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='students', catalog='spark_catalog', namespace=['test'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4c37ad-2900-48e3-adbb-027ef861df3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Через Spark SQL можно создавать таблицы и вставлять записи\n",
    "spark.sql(\"CREATE SCHEMA test\")\n",
    "spark.sql(\"CREATE TABLE test.students (name VARCHAR(64), address VARCHAR(64)) USING PARQUET PARTITIONED BY (student_id INT)\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO test.students\n",
    "VALUES ('Bob Brown', '456 Taylor St, Cupertino', 222222)\n",
    "     , ('Cathy Johnson', '789 Race Ave, Palo Alto', 333333)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfece9f-8763-4f87-a2cc-931eeb957123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+----------+\n",
      "|name         |address                 |student_id|\n",
      "+-------------+------------------------+----------+\n",
      "|Cathy Johnson|789 Race Ave, Palo Alto |333333    |\n",
      "|Bob Brown    |456 Taylor St, Cupertino|222222    |\n",
      "+-------------+------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddl_demo_df = spark.sql(\"SELECT * FROM test.students\")\n",
    "ddl_demo_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacfc89-2c21-4f76-9a1c-111b867e5fcf",
   "metadata": {},
   "source": [
    "За дополнительными сведениями об INSERT можно обратиться к [документации](https://spark.apache.org/docs/latest/sql-ref-syntax-dml-insert-table.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b5c2c0-9367-4f9f-9293-3de77420db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Удалить схему test\")\n",
    "spark.sql(\"DROP SCHEMA test CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85043d70-c9e6-487f-9878-e1e0622d8bb2",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b08b51-6dcf-477e-aa67-ffa698963972",
   "metadata": {},
   "source": [
    "[Delta](https://delta.io/learn/getting-started/) - современный формат эффективного хранения частоменяющихся данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65d45f17-002d-4742-b44d-ba69fceb1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить первую версию\n",
    "cars_df \\\n",
    "  .limit(5) \\\n",
    "  .write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c36ef013-80c6-469f-a057-4a70322b56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочитать текущую версию\n",
    "delta_df = spark.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .load(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615fb4fd-2622-4c64-b71e-344f8981e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.createOrReplaceTempView(\"my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10bdf066-ddcf-4a77-bb62-3a722e904aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from my_delta_cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26c48bf-bdf6-471f-ad8f-d24e4438a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# дописать одну строку / записать новую версию\n",
    "cars_df.limit(1) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"../out/my_delta_cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e824ff83-9de3-4e26-9f05-ac159f165090",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from my_delta_cars\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f4ac062-d41a-46ce-8184-247818119814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", 0) \\\n",
    "  .load(\"../out/my_delta_cars\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2786d4-7c42-4f28-b830-23f99e338dc8",
   "metadata": {},
   "source": [
    "# Задания\n",
    "\n",
    "1. Получить список всех сотрудников и их максимальные зарплаты\n",
    "1. Получить список всех сотрудников, кто никогда не был менеджером\n",
    "1. Для каждого сотрудника, найти разницу между их зарплатой (текущей/последней) и максимальной зарплатой в их отделе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba23e3f7-ed7c-4488-bcb4-4d6b364f6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://postgres:5432/spark\"\n",
    "user = \"docker\"\n",
    "password = \"docker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6df03e-56c0-4f16-923d-647471173767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(table_name):\n",
    "    return spark.read. \\\n",
    "        format(\"jdbc\"). \\\n",
    "        option(\"driver\", driver). \\\n",
    "        option(\"url\", url). \\\n",
    "        option(\"user\", user). \\\n",
    "        option(\"password\", password). \\\n",
    "        option(\"dbtable\", \"public.\" + table_name). \\\n",
    "        load()\n",
    "\n",
    "employees_df = read_table(\"employees\")\n",
    "salaries_df = read_table(\"salaries\")\n",
    "dept_managers_df = read_table(\"dept_manager\")\n",
    "dept_emp_df = read_table(\"dept_emp\")\n",
    "departments_df = read_table(\"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "778419e3-c1d4-4713-ac13-5815c7ae68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table names\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "salaries_df.createOrReplaceTempView(\"salaries\")\n",
    "dept_managers_df.createOrReplaceTempView(\"dept_manager\")\n",
    "dept_emp_df.createOrReplaceTempView(\"dept_emp\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
